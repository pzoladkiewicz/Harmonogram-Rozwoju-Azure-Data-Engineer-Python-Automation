<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>


# Harmonogram Rozwoju: Azure Data Engineer + Python Automation
---

## Cel Główny

Celem tego kompleksowego programu rozwoju jest transformacja dotychczasowych, solidnych umiejętności w obszarze analizy danych i pracy z relacyjnymi bazami danych w kierunku nowoczesnego specjalisty **Azure Data Engineer z zaawansowanymi kompetencjami w Pythonie i automatyzacji**. Plan zakłada systematyczne budowanie wiedzy i praktycznych umiejętności niezbędnych do projektowania, wdrażania, zarządzania i optymalizacji skalowalnych rozwiązań do przetwarzania i analizy danych w ekosystemie Microsoft Azure. Poprzez intensywną naukę, praktyczne projekty i ciągłe doskonalenie, dążymy do osiągnięcia poziomu eksperckiego, który pozwoli na samodzielne prowadzenie złożonych projektów data engineering, efektywne wykorzystanie narzędzi chmurowych oraz automatyzację procesów ETL/ELT. Ostatecznym rezultatem ma być nie tylko zdobycie formalnych kwalifikacji (jak certyfikat DP-203), ale przede wszystkim zbudowanie silnego portfolio i pewności siebie w operowaniu najnowszymi technologiami danych, co umożliwi płynne przejście do nowej, satysfakcjonującej roli zawodowej na dynamicznie rozwijającym się rynku IT.

## Założenia

* **Intensywność:** Maksymalnie 15 godzin nauki tygodniowo, rozłożone na dni robocze.
* **Metodyka:** Kursy online (głównie Microsoft Learn, uzupełnione o materiały z YouTube i dokumentację), praktyczne projekty (GitHub), samodzielne eksperymenty, regularne testy sprawdzające wiedzę.
* **Monitorowanie:** Szczegółowy tracker zadań i postępów w Notion (synchronizowany z GitHub przez n8n, jeśli to możliwe), checkpointy dzienne.
* **Język angielski:** Codzienna, kontekstowa nauka poprzez materiały techniczne, dokumentację i opcjonalne kursy.
* **Projekty:** Każdy tydzień i blok kończy się praktycznym projektem lub zadaniem integrującym zdobytą wiedzę, dokumentowanym na GitHub.
* **Minimalizacja "soft skills":** Skupienie na umiejętnościach technicznych. Dokumentacja i opisy techniczne traktowane jako część kompetencji inżynierskich.
* **Format:** Markdown zoptymalizowany pod GitHub, z linkami i checkpointami.

---

## Blok 1: Fundamenty Azure i Python dla Danych (Szacowany czas: 3 miesiące)

**Cel bloku:** Zrozumienie ekosystemu Azure, umiejętność pracy z podstawowymi usługami danych, swobodne operowanie na danych w Pythonie przy użyciu Pandas, rozpoczęcie ścieżki DP-203. Pierwsze skrypty automatyzujące proste zadania na danych i integracje.

**Projekt bloku:** Zbudowanie prostego pipeline'u ETL w Pythonie, który pobiera dane z różnych źródeł (np. pliki CSV, API), transformuje je przy użyciu Pandas i ładuje do Azure SQL Database. Automatyzacja uruchamiania skryptu.

**Tygodnie:**

* **Tydzień 1:** Wprowadzenie do Azure, podstawy Pythona, pierwsze skrypty.
* **Tydzień 2:** Azure Storage, ADF (podstawy), Python (API, JSON, obsługa błędów).
* **Tydzień 3:** Azure Data Factory (transformacje, wyzwalacze), Python (moduły, funkcje, logowanie).
* **Tydzień 4:** Azure SQL Database (zaawansowane), Python (praca z bazami danych, ORM - wprowadzenie).
* **Tydzień 5:** Bezpieczeństwo w Azure, Python (testowanie - wprowadzenie), Git i GitHub (zaawansowane).
* **Tydzień 6:** Azure Monitor i Log Analytics, Python (optymalizacja kodu, dobre praktyki).
* **Tydzień 7:** Wprowadzenie do Azure Synapse Analytics, Python (praca z dużymi zbiorami danych - wprowadzenie).
* **Tydzień 8:** Wprowadzenie do Azure Databricks, Python (podstawy PySpark).
* **Tydzień 9:** Przygotowanie do DP-203 (część 1), Python (projekty integrujące z Azure SDK).
* **Tydzień 10:** Przygotowanie do DP-203 (część 2), Python (zaawansowane struktury danych i algorytmy).
* **Tydzień 11:** Powtórka materiału DP-203, testy próbne, Python (refaktoryzacja projektów).
* **Tydzień 12:** Egzamin DP-203 (planowany), podsumowanie bloku, finalizacja projektu bloku.

---

## Blok 2: Inżynieria Danych w Azure – Zaawansowane Usługi i Automatyzacja (Szacowany czas: 4 miesiące)

**Cel bloku:** Głębokie opanowanie kluczowych usług Azure Data Engineering (ADF, Synapse, Databricks), umiejętność projektowania i implementowania złożonych potoków danych, zaawansowana automatyzacja zadań z Pythonem oraz integracja z narzędziami DataOps. Zbudowanie solidnego portfolio projektów demonstrujących te umiejętności.

**Projekt bloku:** Stworzenie kompleksowego rozwiązania data pipeline w Azure, które przetwarza dane wsadowe i strumieniowe (wprowadzenie), wykorzystuje zaawansowane transformacje w ADF/Synapse/Databricks, integruje się z systemami monitorowania i jest wdrażane z użyciem podstawowych praktyk CI/CD (np. z Azure DevOps Repos/Pipelines).

**Tygodnie:**

* **Tydzień 13:** Zaawansowane Azure Data Factory: Dynamiczne treści, zmienne, parametry.
* **Tydzień 14:** ADF: Pętle i warunki, iteracyjne przetwarzanie danych.
* **Tydzień 15:** ADF: Wyzwalacze zdarzeń (Event Triggers), integracja z Azure Logic Apps (wprowadzenie).
* **Tydzień 16:** ADF: Zarządzanie środowiskami (Dev, Test, Prod), podstawy CI/CD dla ADF z Azure DevOps.
* **Tydzień 17:** Zaawansowane Azure Synapse Analytics: Optymalizacja zapytań SQL w dedykowanych pulach.
* **Tydzień 18:** Synapse Analytics: Apache Spark w Synapse (architektura, podstawowe operacje, Synapse Notebooks).
* **Tydzień 19:** Synapse Analytics: Praca z Delta Lake w Synapse Spark, zarządzanie tabelami Delta.
* **Tydzień 20:** Synapse Analytics: Integracja z Power BI, bezpieczeństwo w Synapse Workspace.
* **Tydzień 21:** Zaawansowane Azure Databricks: Architektura klastrów, optymalizacja zadań Spark.
* **Tydzień 22:** Databricks: Praca z Delta Lake (ACID, Time Travel, Optymalizacje).
* **Tydzień 23:** Databricks: Strukturyzowane Przetwarzanie Strumieniowe (Structured Streaming) – wprowadzenie.
* **Tydzień 24:** Databricks: Zarządzanie zadaniami (Jobs), podstawy MLflow dla śledzenia eksperymentów (kontekstowo).
* **Tydzień 25:** Python: Zaawansowane wykorzystanie Azure SDK do zarządzania zasobami ADF, Synapse, Databricks.
* **Tydzień 26:** Python: Tworzenie i wdrażanie Azure Functions w Pythonie do zadań przetwarzania danych.
* **Tydzień 27:** Python: Budowanie własnych bibliotek/pakietów Pythona do reużywalnych zadań ETL.
* **Tydzień 28:** Podsumowanie Bloku 2, finalizacja projektu bloku, prezentacja projektu (np. nagranie demo).

---

**Optymalny Moment na Aktywne Szukanie Pracy:**

Po ukończeniu **Bloku 2** (czyli po około 7 miesiącach nauki) i zdaniu egzaminu DP-203 (co ma miejsce na końcu Bloku 1), będziesz posiadał solidne portfolio projektów (projekt z Bloku 1 i bardziej zaawansowany z Bloku 2) oraz udokumentowane umiejętności w kluczowych usługach Azure Data Engineering i Pythonie. To jest **bardzo dobry moment na rozpoczęcie aktywnego, intensywnego szukania pracy** na stanowiskach Mid/Senior Azure Data Engineer. Wieloletnie doświadczenie w pracy z danymi, połączone z nowymi, poszukiwanymi kompetencjami chmurowymi i certyfikacją, będzie silnym atutem.

---

## Blok 3: Specjalizacja, DataOps i Udoskonalanie Portfolio (Szacowany czas: 3-5 miesięcy)

**Cel bloku:** Pogłębienie wiedzy w wybranym obszarze specjalizacji (np. real-time data, data governance, MLOps), opanowanie narzędzi i praktyk DataOps, w tym infrastruktury jako kodu (IaC), oraz stworzenie 1-2 dodatkowych, imponujących projektów do portfolio, które demonstrują pełen zakres zdobytych umiejętności i gotowość do samodzielnego prowadzenia złożonych inicjatyw. Aktywne uczestnictwo w procesach rekrutacyjnych.

**Projekt bloku:** Stworzenie i wdrożenie co najmniej jednego bardzo zaawansowanego projektu data engineering na GitHub, który rozwiązuje realny problem biznesowy (np. system rekomendacji, platforma do analizy sentymentu w czasie rzeczywistym, zautomatyzowany system raportowania z elementami self-service BI), z pełną dokumentacją, testami i wdrożeniem CI/CD, wykorzystując praktyki IaC.

**Tygodnie:**

* **Tydzień 29:** Infrastruktura jako Kod (IaC): Wprowadzenie do ARM Templates i Bicep.
* **Tydzień 30:** IaC: Wdrażanie i zarządzanie zasobami Azure Data za pomocą Bicep/ARM.
* **Tydzień 31:** IaC: Podstawy Terraform dla Azure – porównanie z Bicep/ARM.
* **Tydzień 32:** Azure DevOps dla Data Pipelines: Tworzenie potoków CI/CD dla ADF i Azure Functions/Databricks.
* **Tydzień 33:** GitHub Actions dla Data Pipelines: Alternatywa dla Azure DevOps, automatyzacja zadań.
* **Tydzień 34:** Testowanie automatyczne w Data Engineering (np. testy jednostkowe dla kodu Python, testy integracyjne dla pipeline'ów).
* **Tydzień 35:** Konteneryzacja z Dockerem: Wprowadzenie, budowanie obrazów dla aplikacji Pythona (kontekstowo dla Azure).
* **Tydzień 36:** Zaawansowany Python dla DataOps: Tworzenie narzędzi CLI, packaging, zarządzanie zależnościami.
* **Tydzień 37:** Wybór obszaru specjalizacji (np. Real-time Data z Azure Stream Analytics/Event Hubs/Kafka lub Data Governance z Azure Purview lub MLOps z Azure Machine Learning). Rozpoczęcie pogłębionej nauki.
* **Tydzień 38-42:** Intensywna praca nad projektem specjalizacyjnym i głównym projektem portfolio.
* **Tydzień 43-45:** Optymalizacja kosztów i wydajności w Azure – zaawansowane techniki.
* **Tydzień 46-48+:** Finalizacja projektów portfolio, przygotowanie do rozmów kwalifikacyjnych (technicznych i "zachowawczych" – jak opowiadać o projektach), aktywne szukanie pracy, networking.
